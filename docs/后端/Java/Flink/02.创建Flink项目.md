## 创建项目&导入依赖

创建maven项目

导入依赖

```
    <properties>
        <flink.version>1.17.0</flink.version>
    </properties>


    <dependencies>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-java</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-clients</artifactId>
            <version>${flink.version}</version>
        </dependency>
    </dependencies>
```

## 批处理实现WordCount

需求：统计一段文字中，每个单词出现的频次。

思路：批处理基本思路：先逐行读入文件数据，然后将每一行文字拆分成单词；接着按照单词分组，统计每组数据的个数，就是对应单词的频次

（单词，次数）

创建input/word.txt

```
hello flink
hello world
hello java
```

代码

```java
package com.cyj.wc;
import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.api.java.ExecutionEnvironment;
import org.apache.flink.api.java.operators.AggregateOperator;
import org.apache.flink.api.java.operators.DataSource;
import org.apache.flink.api.java.operators.FlatMapOperator;
import org.apache.flink.api.java.operators.UnsortedGrouping;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.util.Collector;
public class BatchWordCount {
    public static void main(String[] args) throws Exception {
//        1. 创建执行环境
        ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
//        2. 从文件读取数据，再按行读取(存储的元素就是每行的文本)
        DataSource<String> lineDS =env.readTextFile("input/word.txt");
//        3. 转换数据格式
        FlatMapOperator<String,Tuple2<String,Long>> wordAndOne=lineDS.flatMap(
                new FlatMapFunction<String, Tuple2<String, Long>>() {
                    public void flatMap(String s, Collector<Tuple2<String, Long>> collector) throws Exception {
                        String [] words=s.split(" ");
                        for(String word:words){
                            collector.collect(Tuple2.of(word,1L));
                        }
                    }
                }
        );
//        4. 按照word进行分组
        UnsortedGrouping<Tuple2<String,Long>> wordAndOneUG=wordAndOne.groupBy(0);
//        5. 分组内聚合统计
        AggregateOperator <Tuple2<String,Long>> sum=wordAndOneUG.sum(1);
//        6. 打印结果
        sum.print();;
    }

}

```

输出

```
(flink,1)
(world,1)
(java,1)
(hello,3)

```

需要注意的是，这种代码的实现方式，是基于DataSet API的，也就是我们对数据的处理转换，是看作数据集来进行操作的。事实上Flink本身是流批统一的处理架构，批量的数据集本质上也是流，没有必要用两套不同的API来实现。

所以从Flink 1.12开始，官方推荐的做法是**直接使用DataStream API**，在**提交任务时通过将执行模式设为BATCH来进行批处理**：

`$ bin/flink run -Dexecution.runtime-mode=BATCH BatchWordCount.jar`

这样，DataSet API就没什么用了，在实际应用中我们只要维护一套DataStream API就可以。这里只是为了方便大家理解，我们依然用DataSet API做了批处理的实现。

## 流处理实现WordCount

对于Flink而言，流才是整个处理逻辑的底层核心，所以流批统一之后的DataStream API更加强大，可以直接处理批处理和流处理的所有场景

```
package com.cyj.wc;

import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.java.functions.KeySelector;
import org.apache.flink.api.java.operators.DataSource;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.datastream.KeyedStream;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.util.Collector;

public class StreamWordCount {
    public static void main(String[] args) throws Exception {
//        1. 创建流式执行环境
        StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment();
//        2. 读取文件
        DataStreamSource<String> lineDS =env.readTextFile("input/word.txt");
//        3. 转换、分组、求和，得到统计结果
//        3.1 切分，转换
        SingleOutputStreamOperator<Tuple2<String,Integer>> wordAndOneDS=lineDS.flatMap(new FlatMapFunction<String, Tuple2<String, Integer>>() {
            @Override
            public void flatMap(String s, Collector<Tuple2<String, Integer>> collector) throws Exception {
                // 按照空格切分
                String [] words=s.split(" ");
                for(String word:words){
                    // 转换成二元组(word,1)
                    Tuple2<String,Integer> wordAndOne=Tuple2.of(word,1);
                    // 通过 采集器 向下发送数据
                    collector.collect(wordAndOne);
                }
            }
        });
//        3.2 分组
        KeyedStream<Tuple2<String,Integer>,String> wordAndOneKS=wordAndOneDS.keyBy(
                new KeySelector<Tuple2<String, Integer>, String>() {
                    @Override
                    public String getKey(Tuple2<String, Integer> stringIntegerTuple2) throws Exception {
                        return stringIntegerTuple2.f0;
                    }
                }
        );
//        3.3 聚合
        SingleOutputStreamOperator<Tuple2<String,Integer>> sumDS=wordAndOneKS.sum(1);
//        4. 输出数据
        sumDS.print();
//        5. 执行
        env.execute();
    }

}
```

输出

```
6> (hello,1)
11> (world,1)
4> (java,1)
6> (hello,2)
6> (hello,3)
16> (flink,1)
```

## 批处理和流处理对比

批处理和流处理结果对比

```
批处理：整合一起一起处理
(flink,1)
(world,1)
(java,1)
(hello,3)

流处理：得到一个就处理一个
6> (hello,1)
11> (world,1)
4> (java,1)
6> (hello,2)
6> (hello,3)
16> (flink,1)
```



批处理和流处理的代码处理

- 主要观察与批处理程序BatchWordCount的不同：
- 创建执行环境的不同，流处理程序使用的是StreamExecutionEnvironment。
- 转换处理之后，得到的数据对象类型不同。
- 分组操作调用的是keyBy方法，可以传入一个匿名函数作为键选择器（KeySelector），指定当前分组的key是什么。
- 代码末尾需要调用env的execute方法，开始执行任务。



## 流处理处理无限流

以上的wordCount处理的是有限流

现在处理无限流

模拟这种场景，监听socket端口，然后向该端口不断的发送数据。

```
package com.cyj.unbounded;

import org.apache.flink.api.common.typeinfo.TypeHint;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.util.Collector;


public class SocketStreamWordCount {
    public static void main(String[] args) throws Exception {
//        1. 创建执行环境
        StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment();
//        2. 读取数据：socket
        DataStreamSource<String> socketDS=env.socketTextStream("test1",7777);
//        3. 处理数据：切换、转换、分组、聚合
        SingleOutputStreamOperator<Tuple2<String,Integer>> sum=socketDS
                .flatMap(
                        (String value, Collector<Tuple2<String, Integer>> out)->{
                            String[] words=value.split(" ");
                            for(String word:words){
                                out.collect(Tuple2.of(word,1));
                            }
                        }
                )
                .returns(Types.TUPLE(Types.STRING,Types.INT))
                .keyBy(value->value.f0)
                .sum(1);
//        4. 输出
        sum.print();
//        5. 执行
        env.execute();

    }

}

```

程序启动之后没有任何输出、也不会退出。这是正常的，因为Flink的流处理是事件驱动的，当前程序会一直处于监听状态，只有接收到数据才会执行任务、输出统计结果。

输入数据，Flink才会进行处理，真正的流处理。

输入一条，读取一条，计算一条，处理一条